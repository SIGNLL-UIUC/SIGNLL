{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjIW_wFrqoZO"
   },
   "source": [
    "# 9/26 Notebook - Naive Bayes Classification\n",
    "\n",
    "Hi everyone! In this notebook we'll be taking a look at another interdisciplinary aspect of NLP, the `Niave Bayes Model`, which combines NLP and Statistics to create fairly intelligent models. We'll also get some experience using some external APIs, specifically `Tweepy`, to use in our own data collection\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FsHkldyLqoZT"
   },
   "source": [
    "Objectives:\n",
    "- Gain experience with external `APIs` in Python\n",
    "- Learn the fundamentals of the `Naive Bayes` model\n",
    "- Classify and predict tweets between two people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hmhvn6BzqoZU"
   },
   "source": [
    "To finish this notebook, you'll have to complete the following methods:\n",
    "1. `build_dict()`\n",
    "2. `get_bayes_constants()`\n",
    "3. `calc_word_prob()`\n",
    "4. `calc_likelihood()`\n",
    "5. `build_likelihood_dict()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvSRSd_jqoZV"
   },
   "source": [
    "## Part 1: Accessing the Twitter Data\n",
    "\n",
    "In this example, we'll be looking at the Twitter data from `Kanye West` and `Joe Biden`, to see if we can find any significant and recurrent differences between their uses of language\n",
    "\n",
    "To accomplish this, we need to install `Tweepy`, a library that makes it very simple to access the `Twitter API`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6onLnlxrqoZV"
   },
   "outputs": [],
   "source": [
    "pip install tweepy -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_Luozf9qoZX"
   },
   "source": [
    "Now we'll read in some data that we need to access the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LDjfDF4HqoZY"
   },
   "outputs": [],
   "source": [
    "\n",
    "api_key = \"8O7RGlKmkcJWx0UtWevOiXAXX\"\n",
    "api_secret = \"ZiTnEne5hWo3jPImdK9wy4bmT4R7Jjx6uAPEg0GVKqaqvsROc9\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cgYB4PwTqoZY"
   },
   "source": [
    "The credentials are loaded in, so we can activate the API\n",
    "\n",
    "First let's load in our needed `Tweepy` libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lg61DGycqoZZ"
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import API\n",
    "from tweepy import Cursor\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldxYtUJGqoZa"
   },
   "source": [
    "As well as some additional libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JGHeZ5aOqoZb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DgIpOTXQqoZb"
   },
   "source": [
    "Now we can create an API object to access tweets in real time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AB6oBxDoqoZc"
   },
   "outputs": [],
   "source": [
    "auth = tweepy.AppAuthHandler(api_key, api_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WtAYS3f6qoZc"
   },
   "source": [
    "We're going to be comparing the tweets from `Joe Biden` and `Kanye West`, so let's collect a sample of their most recent tweets\n",
    "\n",
    "We'll define a helper function to help us accomplish this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-uNh_qxeqoZd"
   },
   "outputs": [],
   "source": [
    "def get_tweets(handles, num_tweets):\n",
    "    \n",
    "    # initialize the dictionary\n",
    "    tweet_dict = dict()\n",
    "    \n",
    "    # iterate through each twitter handle\n",
    "    for handle in handles:\n",
    "        # get the tweets\n",
    "        tweets = tweepy.Cursor(api.user_timeline, screen_name = handle, include_rts = False).items(num_tweets)\n",
    "        # iterate through each tweet and add it to the dictionary\n",
    "        for tweet in tweets:\n",
    "            tweet_dict[tweet.id] = [tweet.text, handle]\n",
    "    \n",
    "    # create a pandas dataframe\n",
    "    return pd.DataFrame.from_dict(tweet_dict, orient = \"index\", columns = [\"tweet\", \"handle\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKHbWJ8BqoZd"
   },
   "source": [
    "Now we can run the cell below with our helper function to get all of the tweets\n",
    "\n",
    "*Note: The next cell might take a couple of minutes to finish running*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sPRVV9VAqoZe"
   },
   "outputs": [],
   "source": [
    "tweet_data = get_tweets([\"kanyewest\", \"joebiden\"], 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YfxnN_fbqoZe"
   },
   "source": [
    "Now we'll split the data by candidate, and into testing and training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "63t5UvddqoZe"
   },
   "outputs": [],
   "source": [
    "# split the kanye tweets\n",
    "kanye_tweets = tweet_data[tweet_data[\"handle\"] == \"kanyewest\"].to_numpy()\n",
    "np.random.shuffle(kanye_tweets)\n",
    "kanye_train = kanye_tweets[0:int(0.8 * len(kanye_tweets))]\n",
    "kanye_test = kanye_tweets[int(0.8 * len(kanye_tweets)):]\n",
    "\n",
    "# split the biden tweets\n",
    "biden_tweets = tweet_data[tweet_data[\"handle\"] == \"joebiden\"].to_numpy()\n",
    "np.random.shuffle(biden_tweets)\n",
    "biden_train = biden_tweets[0:int(0.8 * len(biden_tweets))]\n",
    "biden_test = biden_tweets[int(0.8 * len(biden_tweets)):]\n",
    "\n",
    "# combine our data\n",
    "train_data = np.concatenate((kanye_train, biden_train))\n",
    "test_data = np.concatenate((kanye_test, biden_test))\n",
    "\n",
    "# one more shuffle for good measure\n",
    "np.random.shuffle(train_data)\n",
    "np.random.shuffle(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lVXOxC0RqoZf"
   },
   "source": [
    "Finally, we need to do one more split: into inputs and outputs (x and y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kEsNPopgqoZg"
   },
   "outputs": [],
   "source": [
    "# split training data\n",
    "train_data_x = train_data[:, 0]\n",
    "train_data_y = train_data[:, 1]\n",
    "\n",
    "# split testing data\n",
    "test_data_x = test_data[:, 0]\n",
    "test_data_y = test_data[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dL43rUjnqoZg"
   },
   "source": [
    "## Part 2: Setting Up the Naive Bayes Metrics\n",
    "\n",
    "The first thing we need to do is create a dictionary to store the frequency of words for each twitter handle\n",
    "\n",
    "We're going to need some additional libraries from `nltk` to aid us with this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5lDS7QgxqoZg",
    "outputId": "c91942d3-bef3-411c-9792-f0e658fa7e8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import re\n",
    "import string\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GFnWhjHOqoZh"
   },
   "source": [
    "We've also provided a helper function to process the tweet into different, cleaned words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rHMzvqqjqoZh"
   },
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    \n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    \n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    \n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    \n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    \n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "            word not in string.punctuation and\n",
    "            word.isalpha()):  # remove punctuation\n",
    "            tweets_clean.append(word)\n",
    "\n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bXFFmswDqoZh"
   },
   "source": [
    "Recall our schema for the frequency dictionary:\n",
    "\n",
    "`{ \"word\" : [kanye_count, biden_count]}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HTGVyD3rqoZh"
   },
   "source": [
    "Complete the function `build_dict()` below, which does the following:\n",
    "1. Sets the values of `tweet` and `handle`, the 0th and 1st elements of `entry`, respectively\n",
    "2. Sets the value of `is_biden`, which checks if the `handle` is Joe Biden's handle\n",
    "3. Gets `counts` from the dictionary, `[0, 0]` if it doesn't exist\n",
    "4. Increments the appropriate index of `counts`\n",
    "5. Update the `freqs` dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGviqzV6qoZi"
   },
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint for Step 1</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li><code>tweet</code> is the 0th element of <code>entry</code> and <code>handle</code> is the 1st element of <code>entry</code></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "svTMuwBCqoZi"
   },
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint for Step 2</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li><code>is_biden</code> should be true when <code>handle</code> equals <code>\"joebiden\"</code></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mdPkzUKUqoZi"
   },
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint for Step 3</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li><code>counts</code> can be found with <code>freqs.get()</code>, with parameters <code>word</code> and <code>[0, 0]</code></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04RXQpVQqoZi"
   },
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint for Step 4</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Increment this value by 1</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F7sbMZQLqoZj"
   },
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint for Step 5</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Set <code>freqs[word]</code> equal to <code>counts</code></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IKAXVhv7qoZj"
   },
   "outputs": [],
   "source": [
    "def build_dict(tweets):\n",
    "    # declare a dictionary\n",
    "    freqs = dict()\n",
    "    \n",
    "    # iterate through each token in each tweet\n",
    "    for entry in tweets:\n",
    "        \n",
    "        # [your code here] - grab the tweet and handle\n",
    "        tweet = ...\n",
    "        handle = ...\n",
    "        \n",
    "        for word in process_tweet(tweet):\n",
    "            # [your code here] - check if the tweet is a biden tweet\n",
    "            is_biden = ...\n",
    "            \n",
    "            # [your code here] - get the tweet from the dictionary, [0, 0] if it doesn't exist\n",
    "            counts = ...\n",
    "            # [your code here] - increment the count\n",
    "            counts[is_biden] += ...\n",
    "            \n",
    "            # [your code here] - update the dictionary\n",
    "            freqs[word] = ...\n",
    "    \n",
    "    # return the dictionary\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h20JqMjWqoZj"
   },
   "source": [
    "Let's store this dictionary in `word_freqs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VMzbDWlvqoZj"
   },
   "outputs": [],
   "source": [
    "\n",
    "word_freqs = build_dict(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iSjrySSHqoZk",
    "outputId": "4d4125f6-c3dc-424b-fc1e-054af2b02a24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lookin' good\n"
     ]
    }
   ],
   "source": [
    "# run this cell to test your code \n",
    "# (by the time you run this cell the tweet counts might have changed, but I tried to make it failry general)\n",
    "if (word_freqs[\"kanye\"][0] >= 10 and word_freqs[\"trump\"][1] >= 144):\n",
    "    print(\"Lookin' good\")\n",
    "else:\n",
    "    print(\"Looking bad bad oh no so bad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMFewRQYqoZk"
   },
   "source": [
    "To calculate the probability a given word is a \"Kanye Word\" or \"Biden Word\", we need normally need the following formulas:\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/gif.latex?\\dpi{200}&space;P(word_{kanye})&space;=&space;\\frac{freq_{word_{kanye}}}{count_{kanye}}\" title=\"P(word_{kanye}) = \\frac{freq_{word_{kanye}}}{count_{kanye}}\" />\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/gif.latex?\\dpi{200}&space;P(word_{biden})&space;=&space;\\frac{freq_{word_{biden}}}{count_{biden}}\" title=\"P(word_{biden}) = \\frac{freq_{word_{biden}}}{count_{biden}}\" />\n",
    "\n",
    "To make our model more robust, we'll add a `smoothing` term, changing our formulas to:\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/gif.latex?\\dpi{200}&space;P(word_{kanye})&space;=&space;\\frac{freq_{word_{kanye}}&space;&plus;&space;1}{count_{kanye}&space;&plus;&space;count_{unique}}\" title=\"P(word_{kanye}) = \\frac{freq_{word_{kanye}} + 1}{count_{kanye} + count_{unique}}\" />\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/gif.latex?\\dpi{200}&space;P(word_{biden})&space;=&space;\\frac{freq_{word_{biden}}&space;&plus;&space;1}{count_{biden}&space;&plus;&space;count_{unique}}\" title=\"P(word_{biden}) = \\frac{freq_{word_{biden}} + 1}{count_{biden} + count_{unique}}\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5-yxXYQqoZk"
   },
   "source": [
    "Complete the function `get_bayes_constants()`, which does the following:\n",
    "1. Finds `num_unique`, the number of unique words in `freqs`\n",
    "2. Sets `word` and `counts` using `item`, representing each entry in `freqs`\n",
    "3. Increments `num_kanye_words` and `num_biden_words` if the word has a `count` greater than 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B4yTKKT5qoZk"
   },
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint for Step 1</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li><code>num_unique</code> would be the length of the dictionary!</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "526zfGkrqoZl"
   },
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint for Step 2</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li><code>word</code> is the 0th element of <code>item</code> and <code>counts</code> is the 1st element of <code>item</code></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gnjGfkC7qoZl"
   },
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints for Step 3</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>You can increment <code>num_kanye_words</code> by <code>counts[0] > 0 </code></li>\n",
    "    <li>You can increment <code>num_biden_words</code> by <code>counts[1] > 0 </code></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XPGJ5ZFMqoZl"
   },
   "outputs": [],
   "source": [
    "def get_bayes_constants(freqs):\n",
    "    # [your code here] - finds the number of unique words in the dictionary\n",
    "    num_unique = ...\n",
    "\n",
    "    # initializes variables\n",
    "    num_kanye_words = 0\n",
    "    num_biden_words = 0\n",
    "    \n",
    "    # itereates through dictionary\n",
    "    for item in freqs.items():\n",
    "        \n",
    "        # [your code here] - gets the word and count\n",
    "        word = ...\n",
    "        counts = ...\n",
    "        \n",
    "        # [your code here] - increments words when appropriate\n",
    "        num_kanye_words += ...\n",
    "        num_biden_words += ...\n",
    "        \n",
    "    # returns values\n",
    "    return (num_unique, num_kanye_words, num_biden_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i1vKm4wSqoZl"
   },
   "outputs": [],
   "source": [
    "# run this cell to test\n",
    "# (by the time you run this cell the tweet counts might have changed, but I tried to make it failry general)\n",
    "num_unique, num_kanye_words, num_biden_words = get_bayes_constants(word_freqs)\n",
    "if (num_unique > 3000 and num_kanye_words > 1500 and num_biden_words > 2000):\n",
    "    print(\"You're an actual lexicographic warlock\")\n",
    "else:\n",
    "    print(\"Better go back to wizarding school :(\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptAx3-pJqoZm"
   },
   "source": [
    "Now we have the values for $num_{unique}$, $count_{kanye}$, and $count_{biden}$, so we can calculate $P(word_{kanye})$ and $P(word_{biden})$ using the formula above\n",
    "\n",
    "The formulas have been repasted below for your convenience\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/gif.latex?\\dpi{200}&space;P(word_{kanye})&space;=&space;\\frac{freq_{word_{kanye}}&space;&plus;&space;1}{count_{kanye}&space;&plus;&space;count_{unique}}\" title=\"P(word_{kanye}) = \\frac{freq_{word_{kanye}} + 1}{count_{kanye} + count_{unique}}\" />\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/gif.latex?\\dpi{200}&space;P(word_{biden})&space;=&space;\\frac{freq_{word_{biden}}&space;&plus;&space;1}{count_{biden}&space;&plus;&space;count_{unique}}\" title=\"P(word_{biden}) = \\frac{freq_{word_{biden}} + 1}{count_{biden} + count_{unique}}\" />\n",
    "\n",
    "Complete the function `calc_word_prob()`, which does the following:\n",
    "1. Finds the value of `count` using `freqs`, returning `[0, 0]` if the word is non existant\n",
    "2. Calculates `word_kanye_numerator` and `word_kanye_denominator` using the formula above\n",
    "3. Calculates `word_biden_numerator` and `word_biden_denominator` using the formula above\n",
    "4. Divides the values in `2` and `3` appropriately to calculate `p_word_kanye` and `p_word_biden`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-sawUXbqoZm"
   },
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint for Step 1</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>You can use the dictionary's <code>get()</code> function with <code>word</code> and <code>[0, 0]</code> as parameters</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aoLEZv37qoZm"
   },
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints for Step 2</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li><code>word_kanye_numerator = (count[0] + 1)</code></li>\n",
    "    <li><code>word_kanye_denominator = num_kanye_words + num_unique</code></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3UZyif4aqoZm"
   },
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints for Step 3</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li><code>word_biden_numerator = (count[1] + 1)</code></li>\n",
    "    <li><code>word_biden_denominator = num_biden_words + num_unique</code></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EgV_oJ_eqoZn"
   },
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints for Step 4</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li><code>p_word_kanye = word_kanye_numerator / word_kanye_denominator</code></li>\n",
    "    <li><code>word_biden_numerator / word_biden_denominator</code></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-RJpzMmkqoZn"
   },
   "outputs": [],
   "source": [
    "def calc_word_prob(word, freqs):\n",
    "    # get our needed constants\n",
    "    num_unique, num_kanye_words, num_biden_words = get_bayes_constants(freqs)\n",
    "    \n",
    "    # [your code here] - gets the count in the dictionary\n",
    "    count = ...\n",
    "    \n",
    "    # [your code here] - calculates the kanye numerator and denominator\n",
    "    word_kanye_numerator = ...\n",
    "    word_kanye_denominator = ...\n",
    "    \n",
    "    # [your code here] - calculates the biden numerator and denominator\n",
    "    word_biden_numerator = ...\n",
    "    word_biden_denominator = ...\n",
    "    \n",
    "    # [your code here] - calculates the probabilities\n",
    "    p_word_kanye = ...\n",
    "    p_word_biden = ...\n",
    "    \n",
    "    # returns the probabilities\n",
    "    return (p_word_kanye, p_word_biden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SApAgKZQqoZn"
   },
   "outputs": [],
   "source": [
    "# run this cell to test your code\n",
    "# (by the time you run this cell the tweet counts might have changed, but I tried to make it failry general)\n",
    "trump_prob = calc_word_prob(\"trump\", word_freqs)\n",
    "kanye_prob = calc_word_prob(\"kanye\", word_freqs)\n",
    "jesus_prob = calc_word_prob(\"kanye\", word_freqs)\n",
    "\n",
    "if (trump_prob[1] > trump_prob[0] and kanye_prob[0] > kanye_prob[1] and jesus_prob[0] > jesus_prob[1]):\n",
    "    print(\"That's some spunky code you got there! (good job)\")\n",
    "else:\n",
    "    print(\"Probabili-deez! iykyk (try again)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m6PsObd0qoZo"
   },
   "source": [
    "There's one more metric we need before we can start training the model, and that's `likelihood`. This value is essentially a ratio between how likely a given word is to have come from `Kanye` or `Biden`. We take the `natural log` of this function because it is strictly increasing, so it doesn't change where the maximum occurs\n",
    "\n",
    "Theory aside, the formula for `likelihood` is:\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/gif.latex?\\dpi{300}&space;likelihood&space;=&space;log(\\frac{P(word_{kanye})}{P(word_{biden})})\" title=\"likelihood = log(\\frac{P(word_{kanye})}{P(word_{biden})})\" />\n",
    "\n",
    "Complete the function `calc_likelihood()`, which does the following:\n",
    "\n",
    "1. Stores the probabilties in `probs` by using `calc_word_prob()` \n",
    "2. Uses array indexing to get the values for `kanye_prob` and `biden_prob`\n",
    "3. Calculates `likelihood` using the above formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZdqrG_fIqoZo"
   },
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint for Step 1</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Set <code>prob</code> using the helper function with parameters <code>word</code> and <code>freqs</code></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVprsnuqqoZo"
   },
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint for Step 2</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li><code>kanye_prob</code> is the 0th element of <code>probs</code>, while <code>biden_prob</code> is the first</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QO6k6YfOqoZo"
   },
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints for Step 3</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Use <code>np.log()</code> to take the natural log</li>\n",
    "    <li>Set <code>likelihood</code> equal to the natural log of <code>kanye_prob</code> divide by <code>biden_prob</code>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bKnizYUTqoZo"
   },
   "outputs": [],
   "source": [
    "def calc_likelihood(word, freqs):\n",
    "    # [your code here] - uses the helper function to get the probabilities\n",
    "    probs = ...\n",
    "    \n",
    "    # [your code here] - indexes the kanye and biden probabilities\n",
    "    kanye_prob = ...\n",
    "    biden_prob = ...\n",
    "    \n",
    "    # [your code here] - calculate logprior\n",
    "    likelihood = ...\n",
    "    \n",
    "    # return the value\n",
    "    return likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_sFOYoj6qoZp"
   },
   "outputs": [],
   "source": [
    "# run this cell to test your code\n",
    "# (by the time you run this cell the tweet counts might have changed, but I tried to make it failry general)\n",
    "if calc_likelihood(\"god\", word_freqs) > 0 and calc_likelihood(\"president\", word_freqs) < 0:\n",
    "    print(\"Nice job buster!\")\n",
    "else:\n",
    "    print(\"Back to the lab again, bustee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gL3SehKqoZp"
   },
   "source": [
    "## Part 3: \"Training\" the Model\n",
    "\n",
    "Technically, we're not really training a model per say. There's no regression going on here; all we need to do is create a dictionary that associates every `word` with a `likelihood`\n",
    "\n",
    "Complete the function `build_likelihood_dict()`, which does the following:\n",
    "1. Finds the value of `word` using array indexing with `item`\n",
    "2. Calulates the `likelihood` using the helper function `calc_likelihood`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZhLgLQZqoZp"
   },
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint for Step 1</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li><code>word</code> is the 0th element of <code>item</code></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LoCFx1obqoZp"
   },
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint for Step 2</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Set <code>likelihood</code> equal to the return value of the helper function, with <code>word</code> and <code>freqs</code> as parameters</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MD5SMfC2qoZq"
   },
   "outputs": [],
   "source": [
    "def build_likelihood_dict(freqs):\n",
    "    # intiailizes the dictionary\n",
    "    likelihood_dict = dict()\n",
    "    \n",
    "    # iterates through each item in the dictionary\n",
    "    for item in freqs.items():\n",
    "        # [your code here] - grab the word\n",
    "        word = ...\n",
    "        \n",
    "        # [your code here] - calculate the logprior and append it to the dictionary\n",
    "        likelihood = ...\n",
    "        likelihood_dict[word] = likelihood\n",
    "    \n",
    "    return likelihood_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d_gOLTROqoZq"
   },
   "outputs": [],
   "source": [
    "# store the dictionary we need for testing\n",
    "likelihoods = build_likelihood_dict(word_freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8s7Ro8dqoZq"
   },
   "source": [
    "## Part 4: Testing the Model\n",
    "\n",
    "Now that we have the dictionary we need, we can test our model with our testing data we declared in the beginning. To get the prediction of a new tweet, we'll sum the likelihood of each word in the tweet. Mathematically:\n",
    "\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/gif.latex?\\dpi{300}&space;p&space;=&space;\\sum&space;likelihood(word)\" title=\"p = \\sum likelihood(word)\" />\n",
    "\n",
    "**Note: The way our model was set up, $p < 0$ corresponds to Biden, while $p > 0$ corresponds to Kanye**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fuP92L5YqoZq"
   },
   "source": [
    "Complete the function `predict_tweet()` which does the following:\n",
    "1. Increments the `likelihood_sum` using the `likelihoods_dict`\n",
    "2. Inserts the appropriate conditional to decide between `Kanye` and `Biden`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WCncRFJqoZr"
   },
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint for Step 1</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Increment the value by <code>likelihoods.get()</code>, with parameters <code>word</code> and <code>0</code></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVpFuDjQqoZr"
   },
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint for Step 2</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>In our case, we want to predict <code>\"Kanye\"</code> if <code>likelihood_sum > 0</code></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YgZRszHoqoZr"
   },
   "outputs": [],
   "source": [
    "def predict_tweet(tweet, likelihoods):\n",
    "    # intiialize the sum\n",
    "    likelihood_sum = 0\n",
    "    \n",
    "    # iterate through each word in the cleaned tweet\n",
    "    for word in process_tweet(tweet):\n",
    "        # [your code here] - increment the sum\n",
    "        likelihood_sum += ...\n",
    "    # [your code here] - get the prediction\n",
    "    prediction = \"Kanye\" if ... else \"Biden\"\n",
    "    \n",
    "    # return the prediction and likelihood in a tuple\n",
    "    return (prediction, likelihood_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgKSGkEbqoZr"
   },
   "source": [
    "In the next cell I made some very generic `Joe Biden` and `Kanye West` tweets which your model should hopefully be able to predict! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UTEVKjqWqoZr"
   },
   "outputs": [],
   "source": [
    "biden_pred = predict_tweet(\"Joe Biden is my name, being the President is my game\", likelihoods)\n",
    "kanye_pred = predict_tweet(\"Kanye west jesus is my lord\", likelihoods)\n",
    "\n",
    "print(f\"The model predicted the first tweet as {biden_pred[0]} with likelihood {biden_pred[1]}\")\n",
    "print(f\"The model predicted the second tweet as {kanye_pred[0]} with likelihood {kanye_pred[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DhQMn26CqoZs"
   },
   "source": [
    "I have created a function below that you can use to test your accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mChoxqoVqoZs"
   },
   "outputs": [],
   "source": [
    "def test_accuracy(tweets, handles, likelihoods):\n",
    "    handles[handles == \"joebiden\"] = \"Biden\"\n",
    "    handles[handles == \"kanyewest\"] = \"Kanye\"\n",
    "    \n",
    "    predict_tweet_vectorized = np.vectorize(predict_tweet)\n",
    "    predictions = predict_tweet_vectorized(tweets, likelihoods)\n",
    "    \n",
    "    return np.average(predictions[0] == handles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nah2wSSnqoZs"
   },
   "outputs": [],
   "source": [
    "train_accuracy = test_accuracy(train_data_x, train_data_y, likelihoods)\n",
    "test_accuracy = test_accuracy(test_data_x, test_data_y, likelihoods)\n",
    "\n",
    "print(f\"The training accuracy is {train_accuracy}\")\n",
    "print(f\"The testing accuracy is {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZiMwjpvqoZs"
   },
   "source": [
    "Feel free to try out your own tweets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tP3WfCknqoZt"
   },
   "outputs": [],
   "source": [
    "tweet = \"Bless up\"\n",
    "print(f\"The model thinks this was tweeted by {predict_tweet(tweet, likelihoods)[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkuSYmNRqoZt"
   },
   "source": [
    "## Analysis\n",
    "\n",
    "The predictions are decent, but if you recall from earlier in the semester, not as precise as the `logistic regression` model. There are a couple of reasons for the mediocre performance. Some of the Twitter data I retrieved was difficult to parse because it contained links and images, so there are probably some tweets I did not clean properly, so that's on me\n",
    "\n",
    "Additionally, this model has some assumptions that are not always true. The main assumption this model makes is that all of the data is independent. However, with something like twitter data, it is entirely possible that the two tweets are similar because the same thing was happening at the same time when they were tweeting (like the election!)\n",
    "\n",
    "One good thing about this model is that it is not susceptible to outliers as easily as the logistic model, so definitely keep it in mind if you want an easy and fast model to make decent predictions!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "1gL3SehKqoZp",
    "J8s7Ro8dqoZq",
    "qkuSYmNRqoZt"
   ],
   "name": "Naive Bayes Classifier Blank.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
