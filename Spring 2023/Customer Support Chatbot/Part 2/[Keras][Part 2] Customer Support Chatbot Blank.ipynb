{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2/27 Notebook - Customer Support Chatbot\n",
    "\n",
    "Hello and welcome to this week's notebook! Last week, we learned how to create a custom data set, professionally clean data, and train a chat bot using a bag-of-words model. This week, we'll be making predictions from our model and creating a user interface for our chatbot\n",
    "\n",
    "The solutions to last week's notebook are incorporated into this notebook, so feel free to look through the methods to refresh yourself on how everything works. There will be a heading indicating where the new code begins\n",
    "\n",
    "There's nothing you need to change in the first part, but if you created a custom `intents.json`, make sure you replace that file here\n",
    "\n",
    "**Note: This notebook does requires the additional installation of `Keras` and `Tensorflow`. If you don't want to install these libraries or are having trouble, try the other notebook!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the methods you need to complete for the notebook:\n",
    "1. `predict_tag()`\n",
    "2. `get_response()`\n",
    "3. `chat()`\n",
    "\n",
    "**Note: You can skip reading many of the following cells, but make sure you run them so your model is trained**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by importing our libraries as always. Make sure you run the cell with `pip install nltk`, which will let you download the `nltk` library we'll be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import our nltk libraries\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# install specific downloads\n",
    "nltk.download('punkt', quiet = True)\n",
    "nltk.download('wordnet', quiet = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other useful libraries (numpy == üêê)\n",
    "import numpy as np\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Modify your intents\n",
    "\n",
    "The great part about this chat bot is that it is fully customizable! Edit `intents.json` to your liking to create your own bot. Make sure that for each `intent`, you fill out the fields `tag`, `patterns`, and `responses`\n",
    "\n",
    "You can look at my file, `taco-bell-intents.json`, for reference\n",
    "\n",
    "Once you're done, you can continue to run the cells below!\n",
    "\n",
    "**Note: if you're having JSON formatting issues in the next cell, use [this link](https://jsonlint.com) to validate your JSON**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = open(\"intents.json\").read()\n",
    "intents = json.loads(data_file)\n",
    "# when you print, you should see your JSON\n",
    "print(intents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Parsing the JSON\n",
    "\n",
    "We'll practice a common first step in any NLP project, data cleaning\n",
    "\n",
    "First, complete the function `process_words()` which will clean up our words according to the following steps:\n",
    "1. Get the tokens using `nltk.word_tokenize()`\n",
    "2. Set `cleaned_word` equal to the `lemmatized` and `lowercased` word\n",
    "\n",
    "**Note: Make sure you run the cell immediately below this first; it stores values needed in `process_words()`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Set <code>tokens = nltk.word_tokenize(pattern)</code></li>\n",
    "    <li><code>lemmatizer.lemmatize(...)</code> will lemmatize a word</li>\n",
    "    <li>The paremeter of <code>lemmatizer.lemmatize(...)</code> should be <code>word.lower()</code></li>  \n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare needed variables for process_words()\n",
    "ignore_punctuation = [\"?\", \"!\", \".\", \",\"]\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_words(pattern):\n",
    "    # return variable\n",
    "    words = []\n",
    "    # get the tokens using nltk\n",
    "    tokens = nltk.word_tokenize(pattern)\n",
    "    for word in tokens:\n",
    "        # check if the word should be ignored\n",
    "        if word not in ignore_punctuation and word.isalnum():\n",
    "            # clean the word and add it to the list\n",
    "            cleaned_word = lemmatizer.lemmatize(word.lower())\n",
    "            words.append(cleaned_word)\n",
    "    # return the list\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to test your code\n",
    "if (process_words(\"How was your day today?\") == ['how', 'wa', 'your', 'day', 'today']):\n",
    "    print(\"Nice work, sport!\")\n",
    "else:\n",
    "    print(\"Try again, buddy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have `process_words()` to clean our words, we can parse the data from our JSON\n",
    "\n",
    "Complete the method `parse_intents()` which does the following:\n",
    "1. Set the value of `tag` from our `intent`\n",
    "2. Set `tokenized_words` using the helper method in `process_words()`\n",
    "3. Append a tuple of `tokenized_words` and `tag` to `tag_tokens`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Values of a JSON can be extracted using arrays</li>\n",
    "    <li>Let <code>tag = intent[\"tag\"]</code></li>\n",
    "    <li>Let <code>tokenized_words = process_words(pattern)</code></li>\n",
    "    <li>For the third step, the tuple can be appended with <code>tag_tokens.append((tokenized_words, tag))</code></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_intents(intents):\n",
    "    # declare our needed variables\n",
    "    tags = []\n",
    "    all_words = []\n",
    "    tag_tokens = []\n",
    "    response_dict = dict()\n",
    "    \n",
    "    # iterate through each intent\n",
    "    for intent in intents[\"intents\"]:\n",
    "        \n",
    "        # add the noanswer tag to the dictionary (edge case)\n",
    "        if (intent[\"tag\"] == \"noanswer\"):\n",
    "            response_dict[\"noanswer\"] = intent[\"responses\"]\n",
    "        \n",
    "        # if the intent has no patterns, we can skip\n",
    "        if (len(intent[\"patterns\"]) == 0):\n",
    "            continue\n",
    "        \n",
    "        # add the tag to the list of tag\n",
    "        tag = intent[\"tag\"]\n",
    "        tags.append(tag)\n",
    "        \n",
    "        # update the dictionary\n",
    "        response_dict[tag] = intent[\"responses\"]\n",
    "        \n",
    "        # iterate through each pattern\n",
    "        for pattern in intent[\"patterns\"]:\n",
    "            # create our tokenized words\n",
    "            tokenized_words = process_words(pattern)\n",
    "            # add all the tokenized words to our words\n",
    "            all_words.extend(tokenized_words)\n",
    "            # adds a tuple -> (list of tokens, tag) -> to the list\n",
    "            tag_tokens.append((tokenized_words, tag))\n",
    "    # return our values in a tuple\n",
    "    return (np.array(tags), np.array(all_words), np.array(tag_tokens), response_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this cool trick below to remove all duplicates from our arrays (and sort them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call our function\n",
    "tags, all_words, tag_tokens, tag_responses = parse_intents(intents)\n",
    "# sort and remove duplicates\n",
    "tags = np.array(sorted(list(set(tags))))\n",
    "all_words = np.array(sorted(list(set(all_words))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below and take a quick look to make sure that everything makes sense. It's hard for me to test your code without knowing what's in your JSON, but in general:\n",
    "\n",
    "- `tags` should contain a list of all your tags in the JSON, excluding `noanswer`\n",
    "- `all_words` should be a list of all the words in your JSON's patterns. There should be no duplicates or patterns that aren't words\n",
    "- Each entry of `tag_token_mappings` should have two values in a list. The first should be a list of patterns, and the second should be the tag of that pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tags: {0}\".format(tags))\n",
    "print(\"------\")\n",
    "print(\"All Words: {0}\".format(all_words))\n",
    "print(\"------\")\n",
    "print(\"Tag-Token Mappings: {0}\".format(tag_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Creating a Training Set\n",
    "\n",
    "We know from previous lessons that the computer can't train a model without numeric values. To solve this, we'll use the `bag of words` technique we discussed in the Google Sheets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the helper method `build_bag()` which iterates through each `word` in `all_words`, and appends 1 to `bag` if the word is in `all_words`, and 0 otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>The easiest way to do this is by using a simple <code>if else</code> statement</li>\n",
    "    <li>Recall that <code>A in B</code> will return <code>true</code> if the element A is in the iterable object B, and <code>false</code> otherwise</li>\n",
    "    <li>If you're feeling really fancy, you can just write <code>bag.append(1 * (word in tokens))</code></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bag(all_words, tokens):\n",
    "    # reset our current bag\n",
    "    bag = []\n",
    "    for word in all_words:\n",
    "        # add 0/1 if the word is in our token\n",
    "        in_token = (word in tokens)\n",
    "        bag.append(1 * in_token)\n",
    "    return bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to test your code\n",
    "test_all_words = [\"edgar\", \"allen\", \"poe\", \"said\", \"the\", \"raven\", \"was\", \"nevermore\"]\n",
    "test_tokens = [\"quote\", \"the\", \"raven\", \"nevermore\"]\n",
    "if (build_bag(test_all_words, test_tokens) == [0, 0, 0, 0, 1, 1, 0, 1]):\n",
    "    print(\"You crushed it!\")\n",
    "else:\n",
    "    print(\"Ruh roh raggy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the method `build_training_set()` below, which performs the following steps:\n",
    "1. Grabs the value of `tokens`, the first (index 0) element of `tag_token`\n",
    "2. Grabs the value of `tag`, the second (index 1) element of `tag_token`\n",
    "3. Sets `current_bag` using the helper method `build_bag()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>You can get the values of <code>tokens</code> and <code>tag</code> with <code>tag_token[X]</code>, where <code>X</code> is 0 or 1, appropriately</li>\n",
    "    <li>Let <code>current_bag = build_bag(all_words, tokens)</code></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_training_set(tags, all_words, tag_tokens):\n",
    "    # define our variables to return\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "        \n",
    "    # iterate through each tag-token mapping\n",
    "    for tag_token in tag_tokens:\n",
    "        \n",
    "        # grab our needed values\n",
    "        tokens = tag_token[0]\n",
    "        tag = tag_token[1]\n",
    "        \n",
    "        # reset our current bag\n",
    "        current_bag = build_bag(all_words, tokens)\n",
    "            \n",
    "        # update our training inputs\n",
    "        train_x.append(current_bag)\n",
    "        \n",
    "        # set our outputs equal to 1 in the location\n",
    "        train_y.append(1 * (tags == tag))\n",
    "    \n",
    "    # return our values\n",
    "    return (np.array(train_x), np.array(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = build_training_set(tags, all_words, tag_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print your `train_x` and `train_y` values in the following cell. It's hard for me to tell if you did everything correctly since you could be using a custom data set. If you have any questions about the program, feel free to message me on discord!\n",
    "\n",
    "- `train_x` should be dimension `(m, n)` where `m` = # of total patterns and `n` = # words in `all_words`\n",
    "- `train_y` should be dimension `(m, n)` where `m` = # of total patterns and `n` = # tags in `tags`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(\"Training Inputs: {0}\".format(train_x))\n",
    "print(\"-----\")\n",
    "print(\"Training Outputs: {0}\".format(train_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we continue with training, you may notice that our data is very similarly grouped, specifically the training outputs. As you may have thought, this can cause some unwanted bias in our model. To fix this, we'll `shuffle` our training set by using `np.random.permutation()` and some clever array indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffled indexes\n",
    "shuffled_indexes = np.random.permutation(train_x.shape[0])\n",
    "# set new values for train_x and train_y\n",
    "train_x = train_x[shuffled_indexes]\n",
    "train_y = train_y[shuffled_indexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Training Our Model Using Keras/Tensorflow (no coding)\n",
    "\n",
    "We have our cleaned, numeric inputs and outputs (`train_x` and `train_y`), so now what? \n",
    "\n",
    "It's time to train our model!\n",
    "\n",
    "**Note: In this version of the notebook, we'll be using `Tensorflow` and `Keras`. I have some instructions below on how to set this up. If you're still having trouble, switch over to the other notebook as there's no installation required**\n",
    "\n",
    "1. Open `Anaconda Prompt`\n",
    "2. `conda install pip`\n",
    "3. `pip install --upgrade tensorflow`\n",
    "4. `pip install Keras`\n",
    "5. `conda create -n mnist tensorflow keras`\n",
    "6. `conda activate mnist`\n",
    "7. `conda install jupyter`\n",
    "8. `conda list` - verify that you see jupyter, numpy, keras, and tensorflow\n",
    "9. run `jupyter notebook` and open this file again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, your installation worked without too much trouble. If you can run the next cell without any errors, you should be good to go! As always, if you have any questions you can message me on Discord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the installation worked properly, you'll see a message that reads `Using Tensorflow backend.`\n",
    "\n",
    "Now, we'll use `Keras` to create a `Sequential` model. This library makes it very easy for us to create convolutional neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model will use the following architecture:\n",
    "\n",
    "<img src = \"./bag_of_words.PNG\" style=\"width:75%;\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare our model\n",
    "model = Sequential()\n",
    "# add our layers\n",
    "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(train_y[0]), activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may see a lot of unfamiliar terms in this model, so I'll do my best to define what the above cell does:\n",
    "- `model.add(Dense(...))` adds a layer of neurons to our neural network. The number is the size of our network, but can be overriden by `input_shape`\n",
    "- `Dropout(0.5)` adds `regularization` to our model, something we haven't talked about yet. Basically, `regularization` decreases the likelihood of the model overfitting our data. Overfitting occurs when our model can predict our training set very well, but does poorly with new data\n",
    "- `activation = 'relu'` changes the activation function. Before, we were using `sigmoid`, but `relu` is another popular function. You can read more about it [here](https://www.kaggle.com/dansbecker/rectified-linear-units-relu-in-deep-learning)\n",
    "- In many neural networks, the final activation function is `softmax`, which essentially normalizes our data. You can read more about it [here](https://en.wikipedia.org/wiki/Softmax_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create an optimizer using `stochastic gradient descent`. The algorithm we were using in earlier weeks was `batch gradient descent`. The main difference between the two optimizers is that `batch gradient descent` takes the derivative of the entire data set at once, while `stochastic gradient descent` takes the partial derivative of each entry in the data set one at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters `lr`, `decay`, `momentum`, and `nesterov` adjusts how fast our model will train. With these parameters set, our model will train more slowly over time\n",
    "\n",
    "We set our `loss` function to [categorical_crossentropy](https://gombru.github.io/2018/05/23/cross_entropy_loss/), our `optimizer` to `stochastic gradient descent`, and tell the model to print out the `accuracy` during each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGD(lr = 0.01, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = sgd, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Keras` makes it very easy to train our model. We can use `model.fit()` to accomplish this. Some notes about the parameters:\n",
    "- `epochs` is equivalent to our number of iterations\n",
    "- `batch_size` tells our model how often to compute the partial derivatives\n",
    "- setting `verbose` to 1 just displays a progress bar\n",
    "\n",
    "Run the cell below to visualize the training of our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(train_x, train_y, epochs = 500, batch_size = 5, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B: Finishing the Chatbot\n",
    "\n",
    "Now that we have everything we need, we can finish the chatbot!\n",
    "\n",
    "As a reminder, these are the methods you need to complete:\n",
    "1. `predict_tag()`\n",
    "2. `get_response()`\n",
    "3. `chat()`\n",
    "\n",
    "Again, this is quite a short notebook, so feel free to spend some time looking over and finishing the first part if you need to!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Predicting the Tag\n",
    "\n",
    "Last week we created a neural network that took our training data as an input, and matched it to a tag output. Now, we need to predict tags using custom user inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the method `predict_tag()`, which does the following\n",
    "1. Sets `process_input` to the cleaned and tokenized `user_input`\n",
    "2. Sets `bag_input` to the bag representation of `process_input`\n",
    "3. Calculates `pred_tag_values` by using our `model` and the `predict()` function\n",
    "4. Finds the values for `max_value_tag` and `probability`, the maximum index and value, respectively, of `pred_tag_values`\n",
    "5. Gets the value of `pred_tag` using `max_value_tag` and the list of `tags`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint for Step 1</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Use the <code>process_words()</code> helper function with <code>user_input</code> as the parameter</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint for Step 2</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Use the <code>build_bag()</code> helper function with <code>all_words</code> and <code>process_input</code> as the parameters</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints for Step 3</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li><code>model.predict()</code> will get the values you need</li>\n",
    "    <li>The argument for <code>model.predict()</code> should be <code>bag_input</code></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints for Step 4</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li><code>np.argmax()</code> will return the index of the maximum value of a numpy array. <a href = https://numpy.org/doc/stable/reference/generated/numpy.argmax.html>Documentation</a></li>\n",
    "    <li><code>np.max()</code> will return the maximum value of a numpy array. <a href = https://numpy.org/doc/stable/reference/generated/numpy.amax.html>Documentation</a></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints for Step 5</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li><code>pred_tag</code> will be the value of the numpy array <code>tags</code> at index <code>max_value_tag</code></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tag(user_input, model):\n",
    "    # [your code here] - tokenize/clean inputs\n",
    "    process_input = ...\n",
    "    \n",
    "    # [your code here] - build the bag\n",
    "    bag_input = ...\n",
    "    bag_input = np.array([bag_input]) # note: convert to a numpy array\n",
    "    \n",
    "    # [your code here] - get our predicted values\n",
    "    pred_tag_values = ...\n",
    "    pred_tag_values = pred_tag_values[0] # note: flatten the 2-d array\n",
    "    \n",
    "    # [your code here] - get the index and value of the largest probability value\n",
    "    max_value_tag = ...\n",
    "    probability = ...\n",
    "    \n",
    "    # [your code here] - predict the tag and return\n",
    "    pred_tag = ...\n",
    "    return (pred_tag, probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try your own inputs here and make sure that the tag makes sense!\n",
    "# look at the probability for the bot's confidence level\n",
    "custom_input = \"How are you today?\"\n",
    "predict_tag(custom_input, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Getting a Response\n",
    "\n",
    "Now that we have a way to predict our tags, we need to get a user input from this tag "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the method `get_response()`, which does the following\n",
    "1. Sets `pred_tag` and `probability` using our helper method `predict_tag()`\n",
    "2. Gets the correct responses from the `tag_responses` dictionary. If the `probability` is not high enough, it should equal `tag_responses[\"noanswer\"]`\n",
    "3. Sets the boolean value of `should_exit_bot`, which should be `true` if the predicted tag is `\"goodbye\"` or `\"thanks\"`\n",
    "4. Get a random `response` from our `responses`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint for Step 1</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Use the <code>predict_tag()</code> helper function with <code>user_input</code> and <code>thetas</code> as the parameters</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints for Step 2</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Write your code in the format <code>responses = [_____] if [_____] else [_____]</code></li>\n",
    "    <li>The first blank should be the value from the dictionary, <code>tag_responses[pred_tag]</code></li>\n",
    "    <li>The second blank should be the condition when <code>probability > error_margin</code></li>\n",
    "    <li>The final blank should be the invalid response tag, <code>tag_responses[\"noanswer\"]</code></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint for Step 3</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>The value should be <code>true</code> with the goodbye tag, so set <code>should_exit_bot</code> equal to <code>pred_tag == \"goodbye\" or pred_tag == \"thanks\"</code></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint for Step 4</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li><code>np.random.choice()</code> will return a random element from an array. Read more about it <a href = https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html>here</a></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(user_input, model, error_margin):\n",
    "    # [your code here] - get the predicted tag and probability\n",
    "    pred_tag, probability = ...\n",
    "    \n",
    "    # [your code here] - get a list of different responses\n",
    "    responses = ...\n",
    "    \n",
    "    # [your code here] - check if we should exit the bot\n",
    "    should_exit_bot = ...\n",
    "    \n",
    "    # [your code here] - get the response\n",
    "    response = ...\n",
    "    \n",
    "    # return the variables\n",
    "    return (response, should_exit_bot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try your own inputs here and check the responses! (remember the second output should only be true for goodbye messages)\n",
    "custom_input = \"How are you today?\"\n",
    "get_response(custom_input, model, 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Creating the Chat User Interface\n",
    "\n",
    "Everything is set up! We just need to put it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the method `chat()`, which puts it all together:\n",
    "1. Sets `user_input` using Python's `input()` function. I recommend putting in `human_prefix` as the parameter. Read more about it [here](https://www.w3schools.com/python/ref_func_input.asp)\n",
    "2. Finds `response` and `should_exit` using our helper method `get_response`\n",
    "3. Prints the `response` preceded by the `robot_prefix`\n",
    "4. Sets `continue_chat` appropriately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints for Step 1</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>If you set <code>user_input</code> equal to <code>input()</code>, it will automatically store the user's input</li>\n",
    "    <li>Set the parameter of <code>input()</code> to be <code>human_prefix</code> to improve your UI</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint for Step 2</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Call <code>get_response()</code> with <code>user_input</code>, <code>thetas</code> and <code>0.25</code> as the parameters. The last value can be anything, depending on how accurate you want your bot to be</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint for Step 3</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Use string concatenation to combine the strings</li>\n",
    "    <li>Print <code>robot_prefix + response</code> to the console</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint for Step 4</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>We should continue when we shouldn't exit, so set <code>continue_chat = not should_exit</code></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat():\n",
    "    # initialize variables\n",
    "    continue_chat = True\n",
    "    robot_prefix = \"Bot: \"\n",
    "    human_prefix = \"You: \"\n",
    "    \n",
    "    # give an introduction\n",
    "    print(robot_prefix + \"Hi! I am a bot offering support for Taco Bell. Ask me what I can do! To exit, say goodbye\")\n",
    "    print(\"\")\n",
    "    \n",
    "    # continue while the user doesn't say goodbye\n",
    "    while (continue_chat):\n",
    "        # [your code here] - get the user input from the console\n",
    "        user_input = ...\n",
    "        \n",
    "        # [your code here] - get the response and exit condition from the helper function\n",
    "        response, should_exit = ...\n",
    "        \n",
    "        # [your code here] - print the bot's response\n",
    "        print(...)\n",
    "        print(\"\")\n",
    "        \n",
    "        # [your code here] - set the exit condition\n",
    "        continue_chat = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's everything you need! If you made it this far, congratulations, you just made your first chat bot! Run the method below to test it out\n",
    "\n",
    "If you want to add more patterns/responses to the bot, you can modify `intents.json` to your liking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
