{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2/13 Notebook - Customer Support Chatbot\n",
    "\n",
    "Hello and welcome to this week's notebook! Today, we'll be looking at how to create our own, customizable chat bot. Specifically, we'll be creating a custom data set, learning how to professionally clean data, and training a chat bot using a bag-of-words model\n",
    "\n",
    "**Note: This notebook does requires the additional installation of `Keras` and `Tensorflow`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the methods you need to complete for the notebook:\n",
    "1. Edit `intents.json`\n",
    "2. `process_words()`\n",
    "3. `parse_intents()`\n",
    "4. `build_bag()`\n",
    "5. `build_training_set()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by importing our libraries as always. Make sure you run the cell with `pip install nltk`, which will let you download the `nltk` library we'll be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import our nltk libraries\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# install specific downloads\n",
    "nltk.download('punkt', quiet = True)\n",
    "nltk.download('wordnet', quiet = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other useful libraries (numpy == üêê)\n",
    "import numpy as np\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Modify your intents\n",
    "\n",
    "The great part about this chat bot is that it is fully customizable! Edit `intents.json` to your liking to create your own bot. Make sure that for each `intent`, you fill out the fields `tag`, `patterns`, and `responses`\n",
    "\n",
    "You can look at my file, `taco-bell-intents.json`, for reference\n",
    "\n",
    "Once you're done, you can continue to run the cells below!\n",
    "\n",
    "**Note: if you're having JSON formatting issues in the next cell, use [this link](https://jsonlint.com) to validate your JSON**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = open(\"intents.json\").read()\n",
    "intents = json.loads(data_file)\n",
    "# when you print, you should see your JSON\n",
    "print(intents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Parsing the JSON\n",
    "\n",
    "We'll practice a common first step in any NLP project, data cleaning\n",
    "\n",
    "First, complete the function `process_words()` which will clean up our words according to the following steps:\n",
    "1. Get the tokens using `nltk.word_tokenize()`\n",
    "2. Set `cleaned_word` equal to the `lemmatized` and `lowercased` word\n",
    "\n",
    "**Note: Make sure you run the cell immediately below this first; it stores values needed in `process_words()`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Set <code>tokens = nltk.word_tokenize(pattern)</code></li>\n",
    "    <li><code>lemmatizer.lemmatize(...)</code> will lemmatize a word</li>\n",
    "    <li>The paremeter of <code>lemmatizer.lemmatize(...)</code> should be <code>word.lower()</code></li>  \n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare needed variables for process_words()\n",
    "ignore_punctuation = [\"?\", \"!\", \".\", \",\"]\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_words(pattern):\n",
    "    # return variable\n",
    "    words = []\n",
    "    # [your code here] - get the tokens using nltk\n",
    "    tokens = ...\n",
    "    for word in tokens:\n",
    "        # check if the word should be ignored\n",
    "        if word not in ignore_punctuation and word.isalnum():\n",
    "            # [your code here] - clean the word and add it to the list\n",
    "            cleaned_word = ...\n",
    "            words.append(cleaned_word)\n",
    "    # return the list\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to test your code\n",
    "if (process_words(\"How was your day today?\") == ['how', 'wa', 'your', 'day', 'today']):\n",
    "    print(\"Nice work, sport!\")\n",
    "else:\n",
    "    print(\"Try again, buddy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have `process_words()` to clean our words, we can parse the data from our JSON\n",
    "\n",
    "Complete the method `parse_intents()` which does the following:\n",
    "1. Set the value of `tag` from our `intent`\n",
    "2. Set `tokenized_words` using the helper method in `process_words()`\n",
    "3. Append a tuple of `tokenized_words` and `tag` to `tag_tokens`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Values of a JSON can be extracted using arrays</li>\n",
    "    <li>Let <code>tag = intent[\"tag\"]</code></li>\n",
    "    <li>Let <code>tokenized_words = process_words(pattern)</code></li>\n",
    "    <li>For the third step, the tuple can be appended with <code>tag_tokens.append((tokenized_words, tag))</code></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_intents(intents):\n",
    "    # declare our needed variables\n",
    "    tags = []\n",
    "    all_words = []\n",
    "    tag_tokens = []\n",
    "    response_dict = dict()\n",
    "    \n",
    "    # iterate through each intent\n",
    "    for intent in intents[\"intents\"]:\n",
    "        # if the intent has no patterns, we can skip\n",
    "        if (len(intent[\"patterns\"]) == 0):\n",
    "            continue\n",
    "        \n",
    "        # [your code here] - add the tag to the list of tag\n",
    "        tag = ...\n",
    "        tags.append(tag)\n",
    "        \n",
    "        # update the dictionary\n",
    "        response_dict[tag] = intent[\"responses\"]\n",
    "        \n",
    "        # iterate through each pattern\n",
    "        for pattern in intent[\"patterns\"]:\n",
    "            # [your code here] - create our tokenized words\n",
    "            tokenized_words = ...\n",
    "            # add all the tokenized words to our words\n",
    "            all_words.extend(tokenized_words)\n",
    "            # [your code here] - adds a tuple -> (list of tokens, tag) -> to the list\n",
    "            tag_tokens.append(...)\n",
    "    # return our values in a tuple\n",
    "    return (np.array(tags), np.array(all_words), np.array(tag_tokens), response_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this cool trick below to remove all duplicates from our arrays (and sort them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call our function\n",
    "tags, all_words, tag_tokens, tag_responses = parse_intents(intents)\n",
    "# sort and remove duplicates\n",
    "tags = np.array(sorted(list(set(tags))))\n",
    "all_words = np.array(sorted(list(set(all_words))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below and take a quick look to make sure that everything makes sense. It's hard for me to test your code without knowing what's in your JSON, but in general:\n",
    "\n",
    "- `tags` should contain a list of all your tags in the JSON, excluding `noanswer`\n",
    "- `all_words` should be a list of all the words in your JSON's patterns. There should be no duplicates or patterns that aren't words\n",
    "- Each entry of `tag_token_mappings` should have two values in a list. The first should be a list of patterns, and the second should be the tag of that pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tags: {0}\".format(tags))\n",
    "print(\"------\")\n",
    "print(\"All Words: {0}\".format(all_words))\n",
    "print(\"------\")\n",
    "print(\"Tag-Token Mappings: {0}\".format(tag_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Creating a Training Set\n",
    "\n",
    "We know from previous lessons that the computer can't train a model without numeric values. To solve this, we'll use the `bag of words` technique we discussed in the Google Sheets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the helper method `build_bag()` which iterates through each `word` in `all_words`, and appends 1 to `bag` if the word is in `all_words`, and 0 otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>The easiest way to do this is by using a simple <code>if else</code> statement</li>\n",
    "    <li>Recall that <code>A in B</code> will return <code>true</code> if the element A is in the iterable object B, and <code>false</code> otherwise</li>\n",
    "    <li>If you're feeling really fancy, you can just write <code>bag.append(1 * (word in tokens))</code></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bag(all_words, tokens):\n",
    "    # reset our current bag\n",
    "    bag = []\n",
    "    for word in all_words:\n",
    "        # [your code here] - append the appropriate value to the bag\n",
    "    return bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to test your code\n",
    "test_all_words = [\"edgar\", \"allen\", \"poe\", \"said\", \"the\", \"raven\", \"was\", \"nevermore\"]\n",
    "test_tokens = [\"quote\", \"the\", \"raven\", \"nevermore\"]\n",
    "if (build_bag(test_all_words, test_tokens) == [0, 0, 0, 0, 1, 1, 0, 1]):\n",
    "    print(\"You crushed it!\")\n",
    "else:\n",
    "    print(\"Ruh roh raggy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the method `build_training_set()` below, which performs the following steps:\n",
    "1. Grabs the value of `tokens`, the first (index 0) element of `tag_token`\n",
    "2. Grabs the value of `tag`, the second (index 1) element of `tag_token`\n",
    "3. Sets `current_bag` using the helper method `build_bag()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>You can get the values of <code>tokens</code> and <code>tag</code> with <code>tag_token[X]</code>, where <code>X</code> is 0 or 1, appropriately</li>\n",
    "    <li>Let <code>current_bag = build_bag(all_words, tokens)</code></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_training_set(tags, all_words, tag_tokens):\n",
    "    # define our variables to return\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "        \n",
    "    # iterate through each tag-token mapping\n",
    "    for tag_token in tag_tokens:\n",
    "        \n",
    "        # [your code here] - grab our needed values\n",
    "        tokens = ...\n",
    "        tag = ...\n",
    "        \n",
    "        # [your code here] - reset our current bag\n",
    "        current_bag = ...\n",
    "            \n",
    "        # update our training inputs\n",
    "        train_x.append(current_bag)\n",
    "        \n",
    "        # set our outputs equal to 1 in the location\n",
    "        train_y.append(1 * (tags == tag))\n",
    "    \n",
    "    # return our values\n",
    "    return (np.array(train_x), np.array(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = build_training_set(tags, all_words, tag_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print your `train_x` and `train_y` values in the following cell. It's hard for me to tell if you did everything correctly since you could be using a custom data set. If you have any questions about the program, feel free to message me on discord!\n",
    "\n",
    "- `train_x` should be dimension `(m, n)` where `m` = # of total patterns and `n` = # words in `all_words`\n",
    "- `train_y` should be dimension `(m, n)` where `m` = # of total patterns and `n` = # tags in `tags`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(\"Training Inputs: {0}\".format(train_x))\n",
    "print(\"-----\")\n",
    "print(\"Training Outputs: {0}\".format(train_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we continue with training, you may notice that our data is very similarly grouped, specifically the training outputs. As you may have thought, this can cause some unwanted bias in our model. To fix this, we'll `shuffle` our training set by using `np.random.permutation()` and some clever array indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffled indexes\n",
    "shuffled_indexes = np.random.permutation(train_x.shape[0])\n",
    "# set new values for train_x and train_y\n",
    "train_x = train_x[shuffled_indexes]\n",
    "train_y = train_y[shuffled_indexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Training Our Model Using Keras/Tensorflow (no coding)\n",
    "\n",
    "We have our cleaned, numeric inputs and outputs (`train_x` and `train_y`), so now what? \n",
    "\n",
    "It's time to train our model!\n",
    "\n",
    "**Note: In this version of the notebook, we'll be using `Tensorflow` and `Keras`. I have some instructions below on how to set this up. If you're still having trouble, switch over to the other notebook as there's no installation required**\n",
    "\n",
    "1. Open `Anaconda Prompt`\n",
    "2. `conda install pip`\n",
    "3. `pip install --upgrade tensorflow`\n",
    "4. `pip install Keras`\n",
    "5. `conda create -n mnist tensorflow keras`\n",
    "6. `conda activate mnist`\n",
    "7. `conda install jupyter`\n",
    "8. `conda list` - verify that you see jupyter, numpy, keras, and tensorflow\n",
    "9. run `jupyter notebook` and open this file again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, your installation worked without too much trouble. If you can run the next cell without any errors, you should be good to go! As always, if you have any questions you can message me on Discord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the installation worked properly, you'll see a message that reads `Using Tensorflow backend.`\n",
    "\n",
    "Now, we'll use `Keras` to create a `Sequential` model. This library makes it very easy for us to create convolutional neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model will use the following architecture:\n",
    "\n",
    "<img src = \"./bag_of_words.PNG\" style=\"width:75%;\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare our model\n",
    "model = Sequential()\n",
    "# add our layers\n",
    "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(train_y[0]), activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may see a lot of unfamiliar terms in this model, so I'll do my best to define what the above cell does:\n",
    "- `model.add(Dense(...))` adds a layer of neurons to our neural network. The number is the size of our network, but can be overriden by `input_shape`\n",
    "- `Dropout(0.5)` adds `regularization` to our model, something we haven't talked about yet. Basically, `regularization` decreases the likelihood of the model overfitting our data. Overfitting occurs when our model can predict our training set very well, but does poorly with new data\n",
    "- `activation = 'relu'` changes the activation function. Before, we were using `sigmoid`, but `relu` is another popular function. You can read more about it [here](https://www.kaggle.com/dansbecker/rectified-linear-units-relu-in-deep-learning)\n",
    "- In many neural networks, the final activation function is `softmax`, which essentially normalizes our data. You can read more about it [here](https://en.wikipedia.org/wiki/Softmax_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create an optimizer using `stochastic gradient descent`. The algorithm we were using in earlier weeks was `batch gradient descent`. The main difference between the two optimizers is that `batch gradient descent` takes the derivative of the entire data set at once, while `stochastic gradient descent` takes the partial derivative of each entry in the data set one at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters `lr`, `decay`, `momentum`, and `nesterov` adjusts how fast our model will train. With these parameters set, our model will train more slowly over time\n",
    "\n",
    "We set our `loss` function to [categorical_crossentropy](https://gombru.github.io/2018/05/23/cross_entropy_loss/), our `optimizer` to `stochastic gradient descent`, and tell the model to print out the `accuracy` during each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGD(lr = 0.01, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = sgd, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Keras` makes it very easy to train our model. We can use `model.fit()` to accomplish this. Some notes about the parameters:\n",
    "- `epochs` is equivalent to our number of iterations\n",
    "- `batch_size` tells our model how often to compute the partial derivatives\n",
    "- setting `verbose` to 1 just displays a progress bar\n",
    "\n",
    "Run the cell below to visualize the training of our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(train_x, train_y, epochs = 500, batch_size = 5, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the exciting part of the project is having your chatbot make predictions, I'll be extra kind and give you a sneak preview of next week\n",
    "\n",
    "(I know I know this code is really ugly but I did it to try and deter people from trying to move too far ahead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"Give me a fun fact\"\n",
    "random.choice(tag_responses.get(tags[np.argmax(model.predict(np.array([build_bag(all_words, process_words(user_input))]))[0])]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
