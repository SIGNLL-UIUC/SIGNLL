{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "Naive Bayes is a basic and \"naive\" method of classifying text. Despite being less advanced than many modern techniques, it can still be quite powerful, as you will see in this workshop. \n",
    "\n",
    "Each unique word in the corpus will be considered a feature, and the value of that feature for a given sample will be its frequency. For example, if \"the\" occurs 4 times in a sample, the value of the \"the\" feature will be 4.  \n",
    "\n",
    "A critical step in training a Naive Bayes classifier is deciding what qualifies as a unique word. For our purposes, we recommend converting everything to lower case and removing all punctuation, numbers, and whitespace. However, you can decide how strict you want to be. \n",
    "\n",
    "Complete the tokenize function below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>You can use .split() on a string to create a list of tokens separated by whitespace.</li>\n",
    "\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(message):\n",
    "  message = message.lower()\n",
    "  # make sure to remove all extraneous characters\n",
    "  return ... # a list of all the tokens in the message (including duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll use that tokenization function to get the tokens of each message and put them in a list, messages_tokens. At the same time, we'll keep track of all the unique tokens in vocab. This is also a good time to create y, which has the labels for each sample. In this case, the labels represent spam or not spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Use your tokenize function!</li>\n",
    "\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "y = []\n",
    "messages_tokens = []\n",
    "\n",
    "# reads file and creats vocabulary\n",
    "with open('smsspam_raw.data', 'r') as file:\n",
    "  for line in file:\n",
    "    label, message = line.strip().split('\\t', 1)\n",
    "    tokens = ...\n",
    "    messages_tokens.append(tokens) # used later for populating x\n",
    "    vocab.update(tokens)\n",
    "    y.append(1 if label == 'True' else 0)\n",
    "\n",
    "vocab = sorted(vocab)\n",
    "vocab_dict = {word: idx for idx, word in enumerate(vocab)} # to easily get feature's index from a token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code block below takes the messages and vocab information collected above and uses it to create a matrix, x, that we will be able to use for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "features = len(vocab)\n",
    "x = np.zeros((len(messages_tokens), features)) # shape is (samples, features)\n",
    "for i, tokens in enumerate(messages_tokens):\n",
    "    word_count = defaultdict(int)\n",
    "    for token in tokens:\n",
    "        if token in vocab_dict:\n",
    "            word_count[token] += 1\n",
    "    for token, count in word_count.items():\n",
    "        x[i, vocab_dict[token]] = count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training models, it's important to separate training and testing data. If we used all the data on training and did our testing on the same data, it wouldn't be very fair, as the model could easily just remember what the right answers are. We'll use 30% of the data for testing and leave the rest for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as TTS\n",
    "x_train, x_test, y_train, y_test = TTS(x, y, test_size=0.3) # 30% testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our data is split into training and testing, we can go ahead and actually train our model. Use nb.fit on x_train and y_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "nb.fit(..., ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our model is trained, let's see how good it is. Use nb.predict on the test data, and compare that to the actual values in y_test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = ...\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've done everything right, you should be getting pretty good results! Now, feel free to do some experimentation to see if you can improve upon the model. Maybe you can change your tokenizer or remove unnecessary features? Let us know what your best results are!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
